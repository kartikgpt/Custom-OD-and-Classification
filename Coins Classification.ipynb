{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17863,
     "status": "ok",
     "timestamp": 1641811749570,
     "user": {
      "displayName": "KARTIK GUPTA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03955346075083435339"
     },
     "user_tz": -330
    },
    "id": "H-xkV22-UrBP",
    "outputId": "e4016cf9-909b-4405-94ae-3a984e6f7085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dat/\n",
      "    test/\n",
      "        One/\n",
      "        Ten/\n",
      "        Two/\n",
      "        Five/\n",
      "    train/\n",
      "        One/\n",
      "        Ten/\n",
      "        Two/\n",
      "        Five/\n"
     ]
    }
   ],
   "source": [
    "#Function to pretty print the directory structure\n",
    "import os\n",
    "def list_files(startpath):\n",
    "  for root, dirs, files in os.walk(startpath):\n",
    "    level = root.replace(startpath, '').count(os.sep)\n",
    "    indent = ' ' * 4 * (level)\n",
    "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "    subindent = ' ' * 4 * (level + 1)\n",
    "\n",
    "list_files('/Users/kgupta/Desktop/dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qSCUZvPqU7sA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#CNN Architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten, GlobalAvgPool2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1641811752592,
     "user": {
      "displayName": "KARTIK GUPTA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03955346075083435339"
     },
     "user_tz": -330
    },
    "id": "GVUUQQK2VJEb",
    "outputId": "ab67c5ab-68cd-4600-a1e9-8062d5b297fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6755 images belonging to 4 classes.\n",
      "Found 1497 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data Augmentation and Pre-Processing\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, \n",
    "                                   rotation_range=90,  \n",
    "                                   shear_range = 0.2, \n",
    "                                   zoom_range = 0.2, \n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#Training Set\n",
    "training_set = train_datagen.flow_from_directory('/Users/kgupta/Desktop/dat/train', \n",
    "                                                 target_size = (200,200), \n",
    "                                                 batch_size = 32, \n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "#Test Set\n",
    "test_set = test_datagen.flow_from_directory('/Users/kgupta/Desktop/dat/test', \n",
    "                                            target_size = (200,200), \n",
    "                                            batch_size = 32, \n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1641811752993,
     "user": {
      "displayName": "KARTIK GUPTA",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03955346075083435339"
     },
     "user_tz": -330
    },
    "id": "z6ZunecAVOp-",
    "outputId": "48f3315e-66e7-43d2-9f0a-f86482887a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 200, 200, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               20480512  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 20,506,148\n",
      "Trainable params: 20,506,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 16:03:28.398796: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-11 16:03:28.400936: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#CNN Architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten, GlobalAvgPool2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "\n",
    "# CNN network\n",
    "model.add( Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(200,200, 3)) )\n",
    "model.add( MaxPool2D(2,2) )\n",
    "\n",
    "model.add( Conv2D(32, (3,3), activation='relu', padding='same') )\n",
    "model.add( MaxPool2D(2,2) )\n",
    "\n",
    "model.add( Conv2D(64, (3,3), activation='relu', padding='same') )\n",
    "model.add( MaxPool2D(2,2) )\n",
    "# Transition between CNN and MLP\n",
    "model.add(Flatten())\n",
    "\n",
    "# MLP network\n",
    "model.add( Dense(512, activation='relu') )\n",
    "\n",
    "model.add( Dense(4, activation='softmax') )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ehc8awGfVeKI"
   },
   "outputs": [],
   "source": [
    "#Setting up Training Scheme\n",
    "optm = Adam(learning_rate = 1e-3) #Loss fuction\n",
    "\n",
    "model.compile(optimizer = optm, loss = 'categorical_crossentropy', metrics = ['acc']) \n",
    "\n",
    "#Setting Up callbacks\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=5, factor=0.1, verbose=True),\n",
    "    ModelCheckpoint('best_model', save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqHMq1JgVsS-",
    "outputId": "6b2764fe-df7e-43ee-bd8c-2bc7e29ac168"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 16:03:45.762060: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "120/120 [==============================] - 68s 563ms/step - loss: 1.8401 - acc: 0.3446 - val_loss: 0.9438 - val_acc: 0.5184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 16:04:53.945888: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 2/200\n",
      "120/120 [==============================] - 78s 648ms/step - loss: 0.8963 - acc: 0.5903 - val_loss: 0.8439 - val_acc: 0.5845\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 3/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.8189 - acc: 0.6314 - val_loss: 0.8291 - val_acc: 0.6313\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 4/200\n",
      "120/120 [==============================] - 79s 656ms/step - loss: 0.7164 - acc: 0.6806 - val_loss: 0.6618 - val_acc: 0.7168\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 5/200\n",
      "120/120 [==============================] - 82s 685ms/step - loss: 0.6324 - acc: 0.7181 - val_loss: 0.6995 - val_acc: 0.6941\n",
      "Epoch 6/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.6457 - acc: 0.7044 - val_loss: 0.7863 - val_acc: 0.6573\n",
      "Epoch 7/200\n",
      "120/120 [==============================] - 88s 732ms/step - loss: 0.5970 - acc: 0.7293 - val_loss: 0.6910 - val_acc: 0.7081\n",
      "Epoch 8/200\n",
      "120/120 [==============================] - 91s 755ms/step - loss: 0.6076 - acc: 0.7300 - val_loss: 0.5667 - val_acc: 0.7528\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 9/200\n",
      "120/120 [==============================] - 81s 676ms/step - loss: 0.5162 - acc: 0.7668 - val_loss: 0.5945 - val_acc: 0.7629\n",
      "Epoch 10/200\n",
      "120/120 [==============================] - 82s 678ms/step - loss: 0.4569 - acc: 0.8020 - val_loss: 0.5232 - val_acc: 0.7776\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 11/200\n",
      "120/120 [==============================] - 89s 737ms/step - loss: 0.4938 - acc: 0.7961 - val_loss: 0.4785 - val_acc: 0.7956\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 12/200\n",
      "120/120 [==============================] - 90s 748ms/step - loss: 0.4440 - acc: 0.8101 - val_loss: 0.5393 - val_acc: 0.7829\n",
      "Epoch 13/200\n",
      "120/120 [==============================] - 92s 765ms/step - loss: 0.4540 - acc: 0.8064 - val_loss: 0.4768 - val_acc: 0.8190\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 14/200\n",
      "120/120 [==============================] - 87s 727ms/step - loss: 0.4300 - acc: 0.8025 - val_loss: 0.6003 - val_acc: 0.7809\n",
      "Epoch 15/200\n",
      "120/120 [==============================] - 84s 700ms/step - loss: 0.4199 - acc: 0.8239 - val_loss: 0.4690 - val_acc: 0.8049\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 16/200\n",
      "120/120 [==============================] - 88s 733ms/step - loss: 0.3784 - acc: 0.8411 - val_loss: 0.4775 - val_acc: 0.7896\n",
      "Epoch 17/200\n",
      "120/120 [==============================] - 88s 731ms/step - loss: 0.3588 - acc: 0.8451 - val_loss: 0.5145 - val_acc: 0.7822\n",
      "Epoch 18/200\n",
      "120/120 [==============================] - 91s 754ms/step - loss: 0.3464 - acc: 0.8598 - val_loss: 0.5021 - val_acc: 0.8143\n",
      "Epoch 19/200\n",
      "120/120 [==============================] - 93s 777ms/step - loss: 0.4037 - acc: 0.8350 - val_loss: 0.4725 - val_acc: 0.8043\n",
      "Epoch 20/200\n",
      "120/120 [==============================] - 96s 796ms/step - loss: 0.3262 - acc: 0.8623 - val_loss: 0.4318 - val_acc: 0.8176\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 21/200\n",
      "120/120 [==============================] - 91s 761ms/step - loss: 0.2911 - acc: 0.8784 - val_loss: 0.4911 - val_acc: 0.8023\n",
      "Epoch 22/200\n",
      "120/120 [==============================] - 93s 771ms/step - loss: 0.3159 - acc: 0.8744 - val_loss: 0.3815 - val_acc: 0.8564\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 23/200\n",
      "120/120 [==============================] - 90s 750ms/step - loss: 0.2818 - acc: 0.8887 - val_loss: 0.3502 - val_acc: 0.8758\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 24/200\n",
      "120/120 [==============================] - 90s 751ms/step - loss: 0.2771 - acc: 0.8907 - val_loss: 0.5574 - val_acc: 0.7909\n",
      "Epoch 25/200\n",
      "120/120 [==============================] - 88s 732ms/step - loss: 0.2956 - acc: 0.8745 - val_loss: 0.4460 - val_acc: 0.8257\n",
      "Epoch 26/200\n",
      "120/120 [==============================] - 89s 740ms/step - loss: 0.2653 - acc: 0.8899 - val_loss: 0.3974 - val_acc: 0.8357\n",
      "Epoch 27/200\n",
      "120/120 [==============================] - 91s 757ms/step - loss: 0.2337 - acc: 0.9051 - val_loss: 0.4283 - val_acc: 0.8464\n",
      "Epoch 28/200\n",
      "120/120 [==============================] - 91s 759ms/step - loss: 0.2400 - acc: 0.9060 - val_loss: 0.4227 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 29/200\n",
      "120/120 [==============================] - 89s 741ms/step - loss: 0.1967 - acc: 0.9230 - val_loss: 0.2962 - val_acc: 0.8898\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 30/200\n",
      "120/120 [==============================] - 89s 744ms/step - loss: 0.1561 - acc: 0.9376 - val_loss: 0.2955 - val_acc: 0.8904\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 31/200\n",
      "120/120 [==============================] - 92s 765ms/step - loss: 0.1559 - acc: 0.9479 - val_loss: 0.3062 - val_acc: 0.8851\n",
      "Epoch 32/200\n",
      "120/120 [==============================] - 93s 774ms/step - loss: 0.1485 - acc: 0.9460 - val_loss: 0.2906 - val_acc: 0.8958\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 33/200\n",
      "120/120 [==============================] - 95s 788ms/step - loss: 0.1423 - acc: 0.9510 - val_loss: 0.2893 - val_acc: 0.8951\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 34/200\n",
      "120/120 [==============================] - 93s 773ms/step - loss: 0.1451 - acc: 0.9429 - val_loss: 0.3269 - val_acc: 0.8798\n",
      "Epoch 35/200\n",
      "120/120 [==============================] - 93s 775ms/step - loss: 0.1298 - acc: 0.9499 - val_loss: 0.2928 - val_acc: 0.8945\n",
      "Epoch 36/200\n",
      "120/120 [==============================] - 91s 752ms/step - loss: 0.1520 - acc: 0.9429 - val_loss: 0.2943 - val_acc: 0.8918\n",
      "Epoch 37/200\n",
      "120/120 [==============================] - 92s 762ms/step - loss: 0.1279 - acc: 0.9548 - val_loss: 0.2843 - val_acc: 0.9011\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 38/200\n",
      "120/120 [==============================] - 89s 738ms/step - loss: 0.1149 - acc: 0.9587 - val_loss: 0.2881 - val_acc: 0.9011\n",
      "Epoch 39/200\n",
      "120/120 [==============================] - 91s 754ms/step - loss: 0.1320 - acc: 0.9494 - val_loss: 0.3021 - val_acc: 0.8918\n",
      "Epoch 40/200\n",
      "120/120 [==============================] - 90s 750ms/step - loss: 0.1218 - acc: 0.9526 - val_loss: 0.2840 - val_acc: 0.9018\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "Epoch 41/200\n",
      "120/120 [==============================] - 91s 754ms/step - loss: 0.1225 - acc: 0.9543 - val_loss: 0.2948 - val_acc: 0.8945\n",
      "Epoch 42/200\n",
      "120/120 [==============================] - 91s 754ms/step - loss: 0.1250 - acc: 0.9493 - val_loss: 0.3028 - val_acc: 0.8938\n",
      "Epoch 43/200\n",
      "120/120 [==============================] - 90s 746ms/step - loss: 0.1206 - acc: 0.9571 - val_loss: 0.3191 - val_acc: 0.8838\n",
      "Epoch 44/200\n",
      "120/120 [==============================] - 92s 765ms/step - loss: 0.1050 - acc: 0.9592 - val_loss: 0.3083 - val_acc: 0.8925\n",
      "Epoch 45/200\n",
      "120/120 [==============================] - 94s 782ms/step - loss: 0.1070 - acc: 0.9594 - val_loss: 0.3074 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 46/200\n",
      "120/120 [==============================] - 90s 748ms/step - loss: 0.1068 - acc: 0.9601 - val_loss: 0.3003 - val_acc: 0.8911\n",
      "Epoch 47/200\n",
      "120/120 [==============================] - 86s 718ms/step - loss: 0.1197 - acc: 0.9552 - val_loss: 0.2998 - val_acc: 0.8938\n",
      "Epoch 48/200\n",
      "120/120 [==============================] - 83s 689ms/step - loss: 0.1080 - acc: 0.9592 - val_loss: 0.2986 - val_acc: 0.8958\n",
      "Epoch 49/200\n",
      "120/120 [==============================] - 83s 686ms/step - loss: 0.1042 - acc: 0.9635 - val_loss: 0.3016 - val_acc: 0.8938\n",
      "Epoch 50/200\n",
      "120/120 [==============================] - 82s 682ms/step - loss: 0.1028 - acc: 0.9590 - val_loss: 0.3030 - val_acc: 0.8918\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 51/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.1055 - acc: 0.9623 - val_loss: 0.3028 - val_acc: 0.8931\n",
      "Epoch 52/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.0979 - acc: 0.9665 - val_loss: 0.3019 - val_acc: 0.8945\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 82s 678ms/step - loss: 0.1012 - acc: 0.9610 - val_loss: 0.3022 - val_acc: 0.8945\n",
      "Epoch 54/200\n",
      "120/120 [==============================] - 81s 675ms/step - loss: 0.1040 - acc: 0.9639 - val_loss: 0.3002 - val_acc: 0.8958\n",
      "Epoch 55/200\n",
      "120/120 [==============================] - 81s 676ms/step - loss: 0.1118 - acc: 0.9570 - val_loss: 0.3000 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 56/200\n",
      "120/120 [==============================] - 82s 677ms/step - loss: 0.0999 - acc: 0.9586 - val_loss: 0.3000 - val_acc: 0.8958\n",
      "Epoch 57/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.0998 - acc: 0.9643 - val_loss: 0.3001 - val_acc: 0.8958\n",
      "Epoch 58/200\n",
      "120/120 [==============================] - 81s 671ms/step - loss: 0.1121 - acc: 0.9536 - val_loss: 0.3001 - val_acc: 0.8958\n",
      "Epoch 59/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.1077 - acc: 0.9574 - val_loss: 0.3000 - val_acc: 0.8951\n",
      "Epoch 60/200\n",
      "120/120 [==============================] - 81s 676ms/step - loss: 0.1097 - acc: 0.9573 - val_loss: 0.3000 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 61/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.0953 - acc: 0.9687 - val_loss: 0.3000 - val_acc: 0.8951\n",
      "Epoch 62/200\n",
      "120/120 [==============================] - 81s 674ms/step - loss: 0.1088 - acc: 0.9595 - val_loss: 0.3000 - val_acc: 0.8951\n",
      "Epoch 63/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.1040 - acc: 0.9610 - val_loss: 0.3000 - val_acc: 0.8951\n",
      "Epoch 64/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.1021 - acc: 0.9613 - val_loss: 0.3000 - val_acc: 0.8951\n",
      "Epoch 65/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.1026 - acc: 0.9611 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 66/200\n",
      "120/120 [==============================] - 81s 673ms/step - loss: 0.0983 - acc: 0.9683 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 67/200\n",
      "120/120 [==============================] - 81s 671ms/step - loss: 0.1078 - acc: 0.9592 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 68/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.1012 - acc: 0.9641 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 69/200\n",
      "120/120 [==============================] - 81s 671ms/step - loss: 0.1084 - acc: 0.9608 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 70/200\n",
      "120/120 [==============================] - 81s 671ms/step - loss: 0.1105 - acc: 0.9598 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 71/200\n",
      "120/120 [==============================] - 81s 673ms/step - loss: 0.0998 - acc: 0.9658 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 72/200\n",
      "120/120 [==============================] - 80s 666ms/step - loss: 0.1097 - acc: 0.9610 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 73/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.0984 - acc: 0.9656 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 74/200\n",
      "120/120 [==============================] - 80s 669ms/step - loss: 0.1093 - acc: 0.9591 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 75/200\n",
      "120/120 [==============================] - 81s 671ms/step - loss: 0.1054 - acc: 0.9597 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 76/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.1026 - acc: 0.9630 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 77/200\n",
      "120/120 [==============================] - 80s 667ms/step - loss: 0.1043 - acc: 0.9652 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 78/200\n",
      "120/120 [==============================] - 80s 669ms/step - loss: 0.1096 - acc: 0.9563 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 79/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.1049 - acc: 0.9575 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 80/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.0975 - acc: 0.9679 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 81/200\n",
      "120/120 [==============================] - 81s 674ms/step - loss: 0.1016 - acc: 0.9623 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 82/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.1011 - acc: 0.9644 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 83/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.1006 - acc: 0.9644 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 84/200\n",
      "120/120 [==============================] - 80s 667ms/step - loss: 0.1048 - acc: 0.9607 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 85/200\n",
      "120/120 [==============================] - 81s 672ms/step - loss: 0.1123 - acc: 0.9554 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 86/200\n",
      "120/120 [==============================] - 80s 666ms/step - loss: 0.1149 - acc: 0.9542 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 87/200\n",
      "120/120 [==============================] - 80s 665ms/step - loss: 0.1002 - acc: 0.9638 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 88/200\n",
      "120/120 [==============================] - 80s 663ms/step - loss: 0.1161 - acc: 0.9522 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 89/200\n",
      "120/120 [==============================] - 80s 667ms/step - loss: 0.1025 - acc: 0.9645 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 90/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.1052 - acc: 0.9608 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 91/200\n",
      "120/120 [==============================] - 80s 666ms/step - loss: 0.1025 - acc: 0.9620 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 92/200\n",
      "120/120 [==============================] - 82s 682ms/step - loss: 0.1138 - acc: 0.9536 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 93/200\n",
      "120/120 [==============================] - 82s 683ms/step - loss: 0.1040 - acc: 0.9659 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 94/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.0974 - acc: 0.9639 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 95/200\n",
      "120/120 [==============================] - 82s 683ms/step - loss: 0.1004 - acc: 0.9666 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 96/200\n",
      "120/120 [==============================] - 82s 683ms/step - loss: 0.1044 - acc: 0.9631 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 97/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.1059 - acc: 0.9587 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 98/200\n",
      "120/120 [==============================] - 82s 678ms/step - loss: 0.1076 - acc: 0.9597 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 99/200\n",
      "120/120 [==============================] - 83s 687ms/step - loss: 0.1105 - acc: 0.9582 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 100/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.1027 - acc: 0.9665 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 101/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.1083 - acc: 0.9641 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 102/200\n",
      "120/120 [==============================] - 82s 684ms/step - loss: 0.1072 - acc: 0.9613 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 103/200\n",
      "120/120 [==============================] - 82s 679ms/step - loss: 0.1055 - acc: 0.9613 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 104/200\n",
      "120/120 [==============================] - 82s 678ms/step - loss: 0.1040 - acc: 0.9605 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 105/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.1123 - acc: 0.9543 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 82s 685ms/step - loss: 0.1024 - acc: 0.9611 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 107/200\n",
      "120/120 [==============================] - 81s 677ms/step - loss: 0.1015 - acc: 0.9599 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 108/200\n",
      "120/120 [==============================] - 82s 683ms/step - loss: 0.1009 - acc: 0.9639 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 109/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.0894 - acc: 0.9679 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 110/200\n",
      "120/120 [==============================] - 82s 679ms/step - loss: 0.1010 - acc: 0.9628 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00110: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 111/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.0972 - acc: 0.9636 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 112/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.1054 - acc: 0.9624 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 113/200\n",
      "120/120 [==============================] - 82s 686ms/step - loss: 0.1011 - acc: 0.9654 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 114/200\n",
      "120/120 [==============================] - 82s 682ms/step - loss: 0.1073 - acc: 0.9619 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 115/200\n",
      "120/120 [==============================] - 82s 684ms/step - loss: 0.1153 - acc: 0.9588 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 116/200\n",
      "120/120 [==============================] - 83s 689ms/step - loss: 0.1148 - acc: 0.9560 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 117/200\n",
      "120/120 [==============================] - 82s 678ms/step - loss: 0.1061 - acc: 0.9633 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 118/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.0994 - acc: 0.9598 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 119/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.0935 - acc: 0.9637 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 120/200\n",
      "120/120 [==============================] - 82s 683ms/step - loss: 0.1210 - acc: 0.9519 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "Epoch 121/200\n",
      "120/120 [==============================] - 82s 684ms/step - loss: 0.1051 - acc: 0.9593 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 122/200\n",
      "120/120 [==============================] - 82s 684ms/step - loss: 0.1124 - acc: 0.9591 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 123/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.1106 - acc: 0.9545 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 124/200\n",
      "120/120 [==============================] - 81s 677ms/step - loss: 0.0921 - acc: 0.9685 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 125/200\n",
      "120/120 [==============================] - 82s 679ms/step - loss: 0.1002 - acc: 0.9622 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "Epoch 126/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.1088 - acc: 0.9587 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 127/200\n",
      "120/120 [==============================] - 82s 678ms/step - loss: 0.0982 - acc: 0.9583 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 128/200\n",
      "120/120 [==============================] - 82s 678ms/step - loss: 0.1094 - acc: 0.9582 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 129/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.1166 - acc: 0.9543 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 130/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.1019 - acc: 0.9649 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "Epoch 131/200\n",
      "120/120 [==============================] - 82s 679ms/step - loss: 0.1001 - acc: 0.9642 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 132/200\n",
      "120/120 [==============================] - 83s 687ms/step - loss: 0.1046 - acc: 0.9673 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 133/200\n",
      "120/120 [==============================] - 135s 1s/step - loss: 0.0977 - acc: 0.9603 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 134/200\n",
      "120/120 [==============================] - 112s 929ms/step - loss: 0.1084 - acc: 0.9600 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 135/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.0976 - acc: 0.9672 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "Epoch 136/200\n",
      "120/120 [==============================] - 123s 1s/step - loss: 0.1008 - acc: 0.9640 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 137/200\n",
      "120/120 [==============================] - 160s 1s/step - loss: 0.1124 - acc: 0.9573 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 138/200\n",
      "120/120 [==============================] - 158s 1s/step - loss: 0.0951 - acc: 0.9701 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 139/200\n",
      "120/120 [==============================] - 184s 2s/step - loss: 0.1028 - acc: 0.9607 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 140/200\n",
      "120/120 [==============================] - 193s 2s/step - loss: 0.1019 - acc: 0.9654 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "Epoch 141/200\n",
      "120/120 [==============================] - 187s 2s/step - loss: 0.0954 - acc: 0.9664 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 142/200\n",
      "120/120 [==============================] - 129s 1s/step - loss: 0.1066 - acc: 0.9552 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 143/200\n",
      "120/120 [==============================] - 127s 1s/step - loss: 0.1082 - acc: 0.9574 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 144/200\n",
      "120/120 [==============================] - 129s 1s/step - loss: 0.0991 - acc: 0.9664 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 145/200\n",
      "120/120 [==============================] - 128s 1s/step - loss: 0.0987 - acc: 0.9653 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n",
      "Epoch 146/200\n",
      "120/120 [==============================] - 123s 1s/step - loss: 0.0960 - acc: 0.9675 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 147/200\n",
      "120/120 [==============================] - 83s 687ms/step - loss: 0.0948 - acc: 0.9648 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 148/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.0926 - acc: 0.9666 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 149/200\n",
      "120/120 [==============================] - 82s 680ms/step - loss: 0.1062 - acc: 0.9625 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 150/200\n",
      "120/120 [==============================] - 81s 675ms/step - loss: 0.1027 - acc: 0.9675 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 1.0000001428009978e-26.\n",
      "Epoch 151/200\n",
      "120/120 [==============================] - 82s 681ms/step - loss: 0.1108 - acc: 0.9589 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 152/200\n",
      "120/120 [==============================] - 81s 676ms/step - loss: 0.1067 - acc: 0.9623 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 153/200\n",
      "120/120 [==============================] - 81s 676ms/step - loss: 0.1051 - acc: 0.9640 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 154/200\n",
      "120/120 [==============================] - 81s 675ms/step - loss: 0.1113 - acc: 0.9571 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 155/200\n",
      "120/120 [==============================] - 81s 676ms/step - loss: 0.1011 - acc: 0.9634 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 1.000000142800998e-27.\n",
      "Epoch 156/200\n",
      "120/120 [==============================] - 81s 671ms/step - loss: 0.1240 - acc: 0.9456 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 157/200\n",
      "120/120 [==============================] - 81s 675ms/step - loss: 0.0938 - acc: 0.9656 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 158/200\n",
      "120/120 [==============================] - 81s 670ms/step - loss: 0.1058 - acc: 0.9622 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "Epoch 159/200\n",
      "120/120 [==============================] - 80s 669ms/step - loss: 0.1046 - acc: 0.9647 - val_loss: 0.2999 - val_acc: 0.8951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "120/120 [==============================] - 80s 668ms/step - loss: 0.1034 - acc: 0.9609 - val_loss: 0.2999 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 1.0000001235416984e-28.\n",
      "Epoch 161/200\n",
      " 85/120 [====================>.........] - ETA: 20s - loss: 0.0934 - acc: 0.9653"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "history = model.fit(training_set, \n",
    "                    epochs=200, \n",
    "                    validation_data = test_set, \n",
    "                    batch_size=64, \n",
    "                    steps_per_epoch = 120,\n",
    "                    callbacks = callbacks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZLfrOZQVxco"
   },
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "\n",
    "max_val_acc = df_history['val_acc'].max()\n",
    "index = df_history[df_history['val_acc'] == max_val_acc].index\n",
    "_, train_loss, train_acc, val_loss, _, _ = [k for k in df_history.iloc[index[0]]]\n",
    "\n",
    "# Printing the Best Results\n",
    "print('Best Validation Scores:\\n')\n",
    "print('Validation_Accuracy : {}'.format(max_val_acc))\n",
    "print('Validation_Loss : {}'.format(val_loss))\n",
    "print('Training_Accuracy : {}'.format(train_acc))\n",
    "print('Training_Loss : {}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV2u2V55WKYr"
   },
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('/Users/kgupta/Desktop/best_model/')\n",
    "\n",
    "#Taking Some Test Images\n",
    "test = {'Images' : [],\n",
    "        'Input' : []}\n",
    "from PIL import Image\n",
    "import os\n",
    "dir_path = '/Users/kgupta/Desktop/r/'\n",
    "\n",
    "for file in os.listdir(dir_path):\n",
    "  #Load Image\n",
    "  im = Image.open(dir_path + file)\n",
    "\n",
    "  #Resize \n",
    "  im = im.resize((400,300))\n",
    "\n",
    "  #Store Sized Image File\n",
    "  test['Images'].append(im)\n",
    "\n",
    "  #Rescale and Store normalized array\n",
    "  im = Image.open(dir_path + file)\n",
    "  im = im.resize((200,200))\n",
    "  im = np.asarray(im)\n",
    "  im = im/255\n",
    "  im = np.expand_dims(im,axis=0)\n",
    "  test['Input'].append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UXTreSKWKWK"
   },
   "outputs": [],
   "source": [
    "# Plotting Images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,7),dpi=100)\n",
    "rows, columns = 3,3\n",
    "\n",
    "for i in range(len(test['Images'])):\n",
    "  fig.add_subplot(rows, columns, i+1)\n",
    "  plt.axis('off')\n",
    "  plt.title('Image '+str(i + 1))\n",
    "  plt.imshow(test['Images'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJdXqrdbWKTy"
   },
   "outputs": [],
   "source": [
    "Class_labels = os.listdir('/Users/kgupta/Desktop/dat/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuNuaXCdWKRM"
   },
   "outputs": [],
   "source": [
    "#Getting Prediction\n",
    "prediction_labels = []\n",
    "for inp_img in test['Input']:\n",
    "  pred_prob = model.predict(inp_img)\n",
    "  prediction = Class_labels[np.argmax(pred_prob)]\n",
    "  prediction_labels.append(prediction)\n",
    "\n",
    "#Plotting Results\n",
    "fig = plt.figure(figsize=(10,9),dpi=100)\n",
    "rows, columns = 3,3\n",
    "\n",
    "for i,pred in zip(range(6),prediction_labels):\n",
    "  fig.add_subplot(rows, columns, i+1)\n",
    "  plt.axis('off')\n",
    "  plt.title('Image '+str(i + 1)+'\\n Prediction : {} Rupee'.format(pred))\n",
    "  plt.imshow(test['Images'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBA9lnmTwLA3QAgLw94/pS",
   "collapsed_sections": [],
   "mount_file_id": "1lO1jWLrTZL6Q3f-XnCihzPPicc368InL",
   "name": "Coins Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
